---
title: "ACIC 2016 simulations"
subtitle: "Dorie et al. (2019)"
format: pdf
---

```{r}
# Load packages
# devtools::install_github("tlverse/tlverse")
#library(dplyr)
library(data.table)
library(estimatr) # Fast regression and Lin
library(gKRLS) # Kernel regression
library(tlverse)
library(sl3) # SuperLearner
library(tmle3) # Targeted Learning
library(cobalt) # To assess covariate balance
library(WeightIt) # For weighting methods
library(DoubleML)
library(ddml)

# For parallel processing (not working)
#library(future)
#ncores <- availableCores()-1
#plan(multicore, workers = ncores)

# Define path for data
path <- "/Users/pablogeraldo/Documents/data_cf_all/"

# Load the covariates
covs <- data.table::fread(paste0(path,"x.csv"),
                          stringsAsFactors = TRUE)

# For now, just for testing, load one iteration of the outcome
#outcomes <- data.table::fread(paste0(path,"1/zymu_13.csv")) 

# First, combine outcome and covs in a single data
#data <- cbind(outcomes, covs)
#data <- data |> dplyr::mutate(y = ifelse(z==0, y0, y1))
```

Here we report the performance of the prognostic propensity score in estimating treatment effects according to the simulations in Dorie et al. (2019), corresponding to the 2016 data competition at the American Causal Inference Conference.

## Simulation setup

We will use the first 20 DGPs (out of 77 different DGPs), corresponding to the "do it yourself" component of the 2016 ACIC competition.

### Premilinaries

We first create the container data.frame to store results across simulations.

```{r}
##########
# STORAGE: create the data.frame to save results
##########

# Create container for the simulation results
# dim: [n_estimators x n_sims x n_dgps] by [n_metrics]

# Define number of simulations and number of GDPs to be tested
n_sims <- 100
n_dgps <- 20

# Define estimators and metrics to be used
estimators <- c("Diff-in-means",
                "Coeff:Linear","Coeff:Linear:Lin","Coeff:Lasso",
                "G-comp:single:Lasso","G-comp:single:xgboost","G-comp:single:earth",
                "G-comp:single:Krls","G-comp:single:SL",
                "G-comp:cross:Lasso","G-comp:cross:xgboost","G-comp:cross:earth",
                "G-comp:cross:Krls","G-comp:cross:SL",
                "Bal:PSM:Logit","Bal:PSM:Lasso","Bal:PSM:xgboost",
                "Bal:PSM:earth","Bal:PSM:Krls","Bal:PSM:SL",
                "Bal:IPW:Logit","Bal:IPW:Lasso","Bal:IPW:xgboost",
                "Bal:IPW:earth","Bal:IPW:Krls","Bal:IPW:SL",
                "Bal:Ebal:raw", "Bal:Ebal:expand",
                "Bal:Yd:Lin","Bal:Yd:Lasso","Bal:Yd:xgboost",
                "Bal:Yd:earth","Bal:Yd:Krls","Bal:Yd:SL","Bal:Yd:all",
                "DR:AIPW:LogLin","DR:AIPW:Lasso",
                "DR:AIPW:xgboost","DR:AIPW:grf",
                "DR:AIPW:earth","DR:AIPW:Krls","DR:AIPW:SL",
                "DR:TMLE:LogLin","DR:TMLE:xgboost","DR:TMLE:grf",
                "DR:TMLE:earth","DR:TMLE:Krls","DR:TMLE:SL",
                "DR:DML:LogLin","DR:DML:xgboost",
                "DR:DML:earth","DR:DML:Krls","DR:DML:SL",
                "PPSM:LinLin","PPSM:LinLog",
                "PPSM:Lasso","PPSM:xgboost",
                "PPSM:earth","PPSM:Krls","PPSM:SL",
                "IPPW:LinLin","IPPW:LinLog",
                "IPPW:Lasso","IPPW:xgboost",
                "IPPW:earth","IPPW:Krls","IPPW:SL"
)
metrics <- c("DGP","Simulation","ATE","ATT","ATC",
             "Estimator","Bias","SE",
             "RMSE","MAE","RESS","Balance","Time")

# Simulation results storage
results <- matrix(NA_real_,
                  nrow = n_sims * n_dgps * length(estimators),
                  ncol = length(metrics)) |> data.table()
names(results) <- metrics
results[, ':='(DGP = rep(1:n_dgps, each = n_sims*length(estimators)),
               Simulation = rep(1:n_sims, each = length(estimators), times = n_dgps),
               Estimator = rep(estimators, times = n_dgps*n_sims))]

# Running times
model_type <- c("Krls:single","SL:single","Krls:cross","SL:cross",
                "Krls:ps","SL:ps","Krls:pps","SL:pps",
                "Bal:SL:ps","Bal:SL:Yd","Bal:SL:pps")
metrics <- c("DGP","Simulation","algorithm","time")
runtimes <- matrix(NA_real_, 
                   nrow = n_sims* n_dgps * length(model_type),
                   ncol = length(metrics)) |> data.table()
names(runtimes) <- metrics
runtimes[, ':='(DGP = rep(1:n_dgps, each = n_sims*length(model_type)),
                Simulation = rep(1:n_sims, each = length(model_type), times = n_dgps),
                algorithm = rep(model_type, times = n_dgps*n_sims))]
```

Now setup the full simulation

```{r, warning=FALSE}
###########################
### --- SIMULATIONS --- ### 
###########################

# For reproducibility
set.seed(24500-03)

# Iterate over different DGPs 
# (1:20), corresponding to the do-it-yourself part of the competition
dgp <- 1:n_dgps
# For now
dgp <- 8

for(i in dgp){
  
  # First, capture the list of files in the directory
  file_list <- 
    list.files(paste0(path,i), # Switch to the i later
               pattern = "*.csv", 
               ignore.case = TRUE,
               full.names = FALSE)
  
  #for(j in 1:length(file_list)){
  for(j in 1:100){
    
    #Keep track of iterations
    #print(j)
    print(paste0(path,i,"/",file_list[j]))
    
    # Create data.frame combining covariates and outcomes
    data <- cbind(
      data.table::fread(
        file = paste0(path,i,"/",file_list[j])), covs
    )[, y := ifelse(z==0, y0, y1)] # Create observed outcome (from switching equation)
    
    # Create data.frames for prediction (g-comp)
    dataZ0 <- copy(data)[,z:=0]
    dataZ1 <- copy(data)[,z:=1]
    
    # Benchmark estimates
    ate <- mean(data$y1 - data$y0)
    att <- mean(data$y1[data$z==1] - data$y0[data$z==1])
    atc <- mean(data$y1[data$z==0] - data$y0[data$z==0])
    dim <- mean(data$y[data$z==1]) - mean(data$y[data$z==0])
    
    # Store the ate, att, and atc for current realization
    results[DGP == i & Simulation == j ,'ATE'] <- ate
    results[DGP == i & Simulation == j ,'ATT'] <- att
    results[DGP == i & Simulation == j ,'ATC'] <- atc
    
    # Store diff-in-means as reference
    results[Estimator == 'Diff-in-means' & 
              DGP == i & Simulation == j ,'Bias'] <- dim - ate
    
    
    ##############################
    ### --- OUTCOME MODELS --- ### 
    ##############################
    
    ###################
    ### OUTCOME MODELS: Stand-alone
    ##################
    # Let's start with the stand alone estimators
    
    #################
    # Lin estimator #
    #################
    
    # Run model
    coeff_lin <- 
      estimatr::lm_lin(y~z, covariates = cobalt::f.build("", names(covs)), data = data)
    
    # Store results
    results[Estimator == 'Coeff:Linear:Lin' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      coeff_lin$coefficients["z"]-ate
    
    # Store predicted values for later use
    data$y1hat_lin <- predict(coeff_lin, type = response, newdata = dataZ1)
    data$y0hat_lin <- predict(coeff_lin, type = response, newdata = dataZ0)
    
    
    ####################################
    # Kernel Regularized Least Squares #
    ####################################
    
    # All vars, including treatment, are splines
    fml <- 
      as.formula(
        paste("y ~",
              paste(paste(c("s(z,"),
                          paste(names(covs), collapse = ",")),
                    c(", bs = \"gKRLS\")"))))
    # Fit the model
    start_time <- proc.time()
    fit_gKRLS <- mgcv::gam(fml, data = data) 
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Krls:single","time"] <- runtime[2]
    
    # Store result: Estimated through g-computation
    results[Estimator == 'G-comp:single:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      calculate_effects(fit_gKRLS, variables = "z")$est-ate
    # Now, store the predicted values for later use
    data$y1hat_krls <- predict(fit_gKRLS, type = "response", newdata = dataZ1)
    data$y0hat_krls <- predict(fit_gKRLS, type = "response", newdata = dataZ0)
    
    # Now fit separate models for each arm
    fml2 <- 
      as.formula(
        paste("y ~",
              paste(paste(c("s("), # No Z indicator, just covariates
                          paste(names(covs), collapse = ",")),
                    c(", bs = \"gKRLS\")"))))
    # Fit the model
    start_time <- proc.time()
    fit_gKRLSy1 <- mgcv::gam(fml2, data = subset(data, z==1)) 
    fit_gKRLSy0 <- mgcv::gam(fml2, data = subset(data, z==0))
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Krls:cross","time"] <- runtime[2]
    
    # Store result: g-computation
    results[Estimator == 'G-comp:cross:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      # V1: model based for whole sample
      (mean(predict.gam(fit_gKRLSy1, newdata = dataZ1))-
         mean(predict.gam(fit_gKRLSy0, newdata = dataZ0)))-ate
    # V2: model based, but for the opposite subset (cross; not really the target)
    #(mean(predict.gam(fit_gKRLSy1, newdata = dataZ1[data$z==0]))-
    #   mean(predict.gam(fit_gKRLSy0, newdata = dataZ0[data$z==1])))-ate
    # V3: combine observed and model based for whole sample (same as v1)
    #mean(c(predict.gam(fit_gKRLSy1, newdata = dataZ1[data$z==0,]),data$y[data$z==1]))-
    #mean(c(predict.gam(fit_gKRLSy0, newdata = dataZ0[data$z==1,]),data$y[data$z==0]))-ate
    
    
    ###################
    ### OUTCOME MODELS: SuperLearner
    ##################
    # Now move into the SL framework, combining multiple estimators
    
    #########
    # TASKS: Specify the tasks to be completed by the SuperLearner
    #########
    # For the SuperLearner, first define the tasks (common across iterations and DGPs)
    # Then train the models in each iteration
    
    # Single regression models (predict Y using covariates)
    task_y <- sl3_Task$new(
      data = data,
      outcome = "y",
      outcome_type = "continuous",
      covariates = c("z", names(covs)))
    
    # Separate regression models (predict Y0, Y1 using covariates)
    task_y0 <- sl3_Task$new(
      data = subset(data, z == 0),
      outcome = "y",
      outcome_type = "continuous",
      covariates = names(covs))
    
    task_y1 <- sl3_Task$new(
      data = subset(data, z == 1),
      outcome = "y",
      outcome_type = "continuous",
      covariates = names(covs))
    
    # PO imputation tasks (for single model, with Z included)
    task_y0hat <- sl3_Task$new(
      data = dataZ0,
      outcome = "y",
      covariates = c("z", names(covs)))  
    
    task_y1hat <- sl3_Task$new(
      data = dataZ1,
      outcome = "y",
      covariates = c("z", names(covs)))  
    
    # PO imputation tasks (for cross-fitted models)
    task_yxhat <- sl3_Task$new(
      data = data,
      outcome = "y",
      covariates = names(covs))  
    
    ######################
    # Instantiate learners to be used
    ######################
    
    # Mean model
    lrn_mean <- Lrnr_mean$new()
    
    # Unregularized linear model, main terms
    lrn_glm <- Lrnr_glm$new()
    
    # Regularized regression (lasso, elastic net, and ridge)
    lrn_lasso <- Lrnr_glmnet$new(alpha = 0)
    #lrn_en025 <- Lrnr_glmnet$new(alpha = 0.25)
    lrn_en050 <- Lrnr_glmnet$new(alpha = 0.5)
    #lrn_en075 <- Lrnr_glmnet$new(alpha = 0.75)
    lrn_ridge <- Lrnr_glmnet$new(alpha = 1)
    
    # Additional, more flexible learners
    lrn_pspl <- Lrnr_polspline$new() # polynomial splines
    lrn_earth <- Lrnr_earth$new() # multivariate adaptive regression splines
    # highly adaptive lasso (very slow! Take out)
    #lrn_hal <- Lrnr_hal9001$new() 
    
    # Tree-based methods
    #lrn_ranger <- Lrnr_ranger$new()
    lrn_xgb <- Lrnr_xgboost$new()
    
    # Add some extra learners
    lrn_gam <- Lrnr_gam$new()
    lrn_bayesglm <- Lrnr_bayesglm$new()
    
    # Not working insider SL (Take out)
    #lrn_krls <- Lrnr_gam$new(fml1) 
    #lrn_krls2 <- Lrnr_gam$new(fml2) 
    #lrn_grf <- Lrnr_grf$new() 
    
    ########################
    # Create learners stack
    ########################
    
    stack <- Stack$new(
      "mean" = lrn_mean, "glm" = lrn_glm, 
      "lasso" = lrn_lasso, "ridge" = lrn_ridge, 
      "en050" = lrn_en050,
      #"en025" = lrn_en025, "en075" = lrn_en075, 
      "polspline" = lrn_pspl, "earth" = lrn_earth, 
      #"ranger" = lrn_ranger, 
      "xgboost" = lrn_xgb,
      "gam" = lrn_gam, "bglm" = lrn_bayesglm)
    
    #####################
    # Instantiate the meta-learners
    #####################
    
    # Non-negative least squares
    sl_nnls <- Lrnr_sl$new(learners = stack,
                           metalearner = Lrnr_nnls$new())
    # Earth
    sl_earth <- Lrnr_sl$new(learners = stack,
                            metalearner = Lrnr_earth$new())
    # xgboost
    sl_xgb <- Lrnr_sl$new(learners = stack,
                          metalearner = Lrnr_xgboost$new())
    
    # Discrete SL: select the best learner (including SL as candidate)
    # Not working for now, so skip CV selector and final step
    # Just use the nnls eSL directly 
    stack_meta <- Stack$new(stack, sl_nnls, sl_earth, sl_xgb)
    cv_selector <- Lrnr_cv_selector$new(eval_function = loss_squared_error)
    dsl <- Lrnr_sl$new(stack_meta, metalearner = cv_selector)
    
    # Not runing the above, only this last ensemble
    esl <- Lrnr_sl$new(stack, metalearner = Lrnr_nnls$new())
    
    #####################
    # Training the stack and meta-learner
    #####################
    
    # First: single model for the outcome
    start_time <- proc.time()
    sl_y <- esl$train(task = task_y)
    # Add here the CV selector later
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "SL:single","time"] <- runtime[2]
    
    # Then: cross-models for the outcome
    # For the treated
    start_time <- proc.time()
    sl_y1 <- esl$train(task = task_y1)
    # For the controls
    sl_y0 <- esl$train(task = task_y0)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "SL:cross","time"] <- runtime[2]
    
    ### SINGLE MODELS ###
    
    # Linear model, main terms
    results[Estimator == 'Coeff:Linear' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      sl_y$learner_fits$glm$coefficients["z"]-ate
    
    # Lasso, treatment term
    results[Estimator == 'Coeff:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      sl_y$learner_fits$lasso$coefficients[2]-ate
    
    # Finally: obtaining predictions, g-computation
    # Both from the SL and also single learners inside the stack
    
    # Lasso, g-comp
    data$y1hat_lasso <- sl_y$learner_fits$lasso$predict(task = task_y1hat)
    data$y0hat_lasso <- sl_y$learner_fits$lasso$predict(task = task_y0hat)
    results[Estimator == 'G-comp:single:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1hat_lasso - data$y0hat_lasso)-ate
    
    # xgboost
    data$y1hat_xgb <- sl_y$learner_fits$xgboost$predict(task = task_y1hat)
    data$y0hat_xgb <- sl_y$learner_fits$xgboost$predict(task = task_y0hat)
    results[Estimator == 'G-comp:single:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1hat_xgb - data$y0hat_xgb)-ate
    
    # Earth
    data$y1hat_earth <- sl_y$learner_fits$earth$predict(task = task_y1hat)
    data$y0hat_earth <- sl_y$learner_fits$earth$predict(task = task_y0hat)
    results[Estimator == 'G-comp:single:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1hat_earth - data$y0hat_earth)-ate
    
    # SuperLearner
    data$y1hat_sl <- sl_y$predict(task = task_y1hat)
    data$y0hat_sl <- sl_y$predict(task = task_y0hat)
    results[Estimator == 'G-comp:single:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1hat_sl - data$y0hat_sl)-ate
    
    # Predictions for all obs
    #mean(y1hat_sl - y0hat_sl)
    # Prediction for the missing PO only (Alternative, not used, almost identical)
    # Exactly identical for the Krls case
    #y1hat_sl_imp <- ifelse(data$z==1, data$y, y1hat_sl)
    #y0hat_sl_imp <- ifelse(data$z==0, data$y, y0hat_sl)
    #mean(y1hat_sl_imp - y0hat_sl_imp)
    
    ### CROSS MODELS ###
    
    # Lasso, g-comp
    data$y1xhat_lasso <- sl_y1$learner_fits$lasso$predict(task = task_yxhat)
    data$y0xhat_lasso <- sl_y0$learner_fits$lasso$predict(task = task_yxhat)
    results[Estimator == 'G-comp:cross:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1xhat_lasso - data$y0xhat_lasso)-ate
    
    # xgboost
    data$y0xhat_xgb <- sl_y0$learner_fits$xgboost$predict(task = task_yxhat)
    data$y1xhat_xgb <- sl_y1$learner_fits$xgboost$predict(task = task_yxhat)
    results[Estimator == 'G-comp:cross:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1xhat_xgb - data$y0xhat_xgb)-ate
    
    # Earth
    data$y1xhat_earth <- sl_y1$learner_fits$earth$predict(task = task_yxhat)
    data$y0xhat_earth <- sl_y0$learner_fits$earth$predict(task = task_yxhat)
    results[Estimator == 'G-comp:cross:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1xhat_earth - data$y0xhat_earth)-ate
    
    # SuperLearner
    data$y1xhat_sl <- sl_y1$predict(task = task_yxhat)
    data$y0xhat_sl <- sl_y0$predict(task = task_yxhat)
    results[Estimator == 'G-comp:cross:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean(data$y1xhat_sl - data$y0xhat_sl)-ate
    
    
    ####################################
    ### --- BALANCING ESTIMATORS --- ### For the predicted Y0
    ####################################
    
    # Yd balance: Lin
    bal_ydhat_lin <- weightit(formula = z ~ y1hat_lin + y0hat_lin,
                              #moments = 3, int = TRUE,
                              method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:Lin' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_lin), data = data)$coefficients["z"]-ate
    
    # Yd balance: Lasso
    bal_ydhat_lasso <- weightit(formula = z ~ y1hat_lasso + y0hat_lasso,
                                #moments = 3, int = TRUE,
                                method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_lasso), data = data)$coefficients["z"]-ate
    
    # Yd balance: xgboost
    bal_ydhat_xgb <- weightit(formula = z ~ y1hat_xgb + y0hat_xgb,
                              #moments = 3, int = TRUE,
                              method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_xgb), data = data)$coefficients["z"]-ate
    
    # Yd balance: earth
    bal_ydhat_earth <- weightit(formula = z ~ y1hat_earth + y0hat_earth,
                                #moments = 3, int = TRUE, 
                                method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_earth), data = data)$coefficients["z"]-ate
    
    # Yd balance: Krls
    bal_ydhat_krls <- weightit(formula = z ~ y1hat_krls + y0hat_krls,
                               #moments = 3, int = TRUE,
                               method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_krls), data = data)$coefficients["z"]-ate
    
    # Yd balance: SL
    start_time <- proc.time()
    bal_ydhat_sl <- weightit(formula = z ~ y1hat_sl + y0hat_sl,
                             #moments = 3, int = TRUE,
                             method = "ebal", data = data)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Bal:SL:Yd","time"] <- runtime[2]
    results[Estimator == 'Bal:Yd:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_sl), data = data)$coefficients["z"]-ate
    
    # Yd balance: All
    bal_ydhat_all <- weightit(formula = z ~ 
                                y1hat_lin + y0hat_lin +
                                y1hat_lasso + y0hat_lasso +
                                y1hat_xgb + y0hat_xgb +
                                y1hat_earth + y0hat_earth +
                                y1hat_krls + y0hat_krls +
                                y1hat_sl + y0hat_sl,
                              #moments = 3, int = TRUE,
                              method = "ebal", data = data)
    results[Estimator == 'Bal:Yd:all' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(bal_ydhat_all), data = data)$coefficients["z"]-ate
    
    
    
    
    ################################
    ### --- TREATMENT MODELS --- ### 
    ################################
    # Now let's focus in propensity score estimation
    
    ############################
    # Linear Probability model # (Potentially misspecified)
    ############################
    
    # Run model and predict probabilities
    data$ps_lpm <- lm(cobalt::f.build("z", names(covs)), data = data) |> 
      predict(type = "response")
    
    ####################################
    # Kernel Regularized Least Squares # (Potentially misspecified)
    ####################################
    
    # All vars are splines
    fmlps <- 
      as.formula(
        paste("z ~",
              paste(paste(c("s("),
                          paste(names(covs), collapse = ",")),
                    c(", bs = \"gKRLS\")"))))
    # Fit the model (incorrect link function)
    fit_gKRLS_lpm <- mgcv::gam(fmlps, data = data)
    # Predicted probabilities
    data$ps_krls_lpm <- predict.gam(fit_gKRLS_lpm, type = "response")
    # Fit the model (appropriate link function)
    start_time <- proc.time()
    fit_gKRLS_binom <- mgcv::gam(fmlps, family = "binomial", data = data)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Krls:ps","time"] <- runtime[2]
    # Predicted probabilities
    data$ps_krls_binom <- predict.gam(fit_gKRLS_binom, type = "response")
    
    #################
    # Super Learner # 
    #################
    
    # New Task: Propensity score estimation (predict treatment given covariates)
    task_ps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = names(covs))
    
    # New stack: remove gam (and bayes glm), very slow for this task 
    stack <- Stack$new(
      "mean" = lrn_mean, "glm" = lrn_glm, 
      "lasso" = lrn_lasso, "ridge" = lrn_ridge,
      "en050" = lrn_en050, 
      #"en025" = lrn_en025, "en075" = lrn_en075,
      "polspline" = lrn_pspl, "earth" = lrn_earth, 
      #"ranger" = lrn_ranger, 
      "xgboost" = lrn_xgb)
    # Note: some learners give extreme propensity scores
    # Earth: some exactly zero
    # polspline: some warnings (model size reduced) and zero probs
    # We stabilize for IPW
    
    # Discrete SL: select the best learner (including SL as candidate)
    stack_meta <- Stack$new(stack, sl_nnls, sl_earth, sl_xgb)
    cv_selector <- Lrnr_cv_selector$new(eval_function = loss_loglik_binomial)
    dsl <- Lrnr_sl$new(stack_meta, metalearner = cv_selector)
    # Same as with outcome models, too slow to use
    # Then run the eSL with nnls (no CV)
    esl <- Lrnr_sl$new(stack, metalearner = Lrnr_nnls$new())
    
    # Now train the new task
    # First: single model for the outcome
    start_time <- proc.time()
    sl_ps <- esl$train(task = task_ps)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "SL:ps","time"] <- runtime[2]
    
    #test <- lrn_XX$train(task = task_ps)
    #summary(test$predict())
    
    ####################################
    ### --- BALANCING ESTIMATORS --- ### Inverse probability weighting
    ####################################
    
    # IPW: logistic propensity score
    data$ps_logit <- sl_ps$learner_fits$glm$predict()
    ipw_logit <- weightit(formula = cobalt::f.build("z", names(covs)),
                          ps = "ps_logit",
                          stabilize = TRUE,
                          data = data)
    results[Estimator == 'Bal:IPW:Logit' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_logit), data = data)$coefficients["z"]-ate
    
    # IPW: Lasso pscore
    data$ps_lasso <- sl_ps$learner_fits$lasso$predict()
    ipw_lasso <- weightit(formula = cobalt::f.build("z", names(covs)),
                          ps = "ps_lasso",
                          stabilize = TRUE,
                          data = data)
    results[Estimator == 'Bal:IPW:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_lasso), data = data)$coefficients["z"]-ate
    
    # IPW: xgboost
    data$ps_xgb <- sl_ps$learner_fits$xgboost$predict()
    ipw_xgb <- weightit(formula = cobalt::f.build("z", names(covs)),
                        ps = "ps_xgb",
                        stabilize = TRUE,
                        data = data)
    results[Estimator == 'Bal:IPW:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_xgb), data = data)$coefficients["z"]-ate
    
    # IPW: earth
    data$ps_earth <- sl_ps$learner_fits$earth$predict()
    ipw_earth <- weightit(formula = cobalt::f.build("z", names(covs)),
                          ps = "ps_earth",
                          stabilize = TRUE,
                          data = data)
    results[Estimator == 'Bal:IPW:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_earth), data = data)$coefficients["z"]-ate
    
    # IPW: krls
    ipw_krls <- weightit(formula = cobalt::f.build("z", names(covs)),
                         ps = "ps_krls_binom",
                         stabilize = TRUE,
                         data = data)
    results[Estimator == 'Bal:IPW:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_krls), data = data)$coefficients["z"]-ate
    
    # IPW: SL 
    data$ps_sl <- sl_ps$predict()
    start_time <- proc.time()
    ipw_sl <- weightit(formula = cobalt::f.build("z", names(covs)),
                       ps = "ps_sl",
                       stabilize = TRUE,
                       data = data)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Bal:SL:ps","time"] <- runtime[2]
    results[Estimator == 'Bal:IPW:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ipw_sl), data = data)$coefficients["z"]-ate
    
    # Now add two calibration weighting
    # Entropy balance with the raw covariates (mean balance)
    # And entropy balance with interactions and higher moments
    # Commenting out for now: weird behavior
    
    # ebal_raw <- weightit(formula = cobalt::f.build("z", names(covs)),
    #                      method = "ebal", estimand = "ATE",
    #                      data = data)
    # results[Estimator == 'Bal:Ebal:raw' & 
    #           DGP == i & Simulation == j ,'Bias'] <- 
    #   lm(y ~ z, weights = get.w(ebal_raw), data = data)$coefficients["z"]-ate
    # 
    # ebal_expand <- weightit(formula = cobalt::f.build("z", names(covs)),
    #                         method = "ebal", estimand = "ATE",
    #                         int = TRUE, moments = 2,
    #                         data = data)
    # results[Estimator == 'Bal:Ebal:expand' & 
    #           DGP == i & Simulation == j ,'Bias'] <- 
    #   lm(y ~ z, weights = get.w(ebal_expand), data = data)$coefficients["z"]-ate
    
    
    ####################################
    ### --- AUGMENTED ESTIMATORS --- ### 
    ####################################
    
    # "DR:AIPW:LogLin"
    results[Estimator == 'DR:AIPW:LogLin' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_lin)/data$ps_logit+data$y1hat_lin)-
             ((1-data$z)*((data$y-data$y0hat_lin)/(1-data$ps_logit))+data$y0hat_lin))-ate
    
    # "DR:AIPW:Lasso"
    results[Estimator == 'DR:AIPW:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_lasso)/data$ps_lasso+data$y1hat_lasso)-
             ((1-data$z)*((data$y-data$y0hat_lasso)/(1-data$ps_lasso))+data$y0hat_lasso))-ate
    
    # "DR:AIPW:xgboost"
    results[Estimator == 'DR:AIPW:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_xgb)/data$ps_xgb+data$y1hat_xgb)-
             ((1-data$z)*((data$y-data$y0hat_xgb)/(1-data$ps_xgb))+data$y0hat_xgb))-ate
    
    # "DR:AIPW:earth" 
    results[Estimator == 'DR:AIPW:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_earth)/data$ps_earth+data$y1hat_earth)-
             ((1-data$z)*((data$y-data$y0hat_earth)/(1-data$ps_earth))+data$y0hat_earth))-ate
    
    # "DR:AIPW:Krls"
    results[Estimator == 'DR:AIPW:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_krls)/data$ps_krls_binom+data$y1hat_krls)-
             ((1-data$z)*((data$y-data$y0hat_krls)/(1-data$ps_krls_binom))+data$y0hat_krls))-ate
    
    # "DR:AIPW:SL"
    results[Estimator == 'DR:AIPW:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      mean((data$z*(data$y-data$y1hat_sl)/data$ps_sl+data$y1hat_sl)-
             ((1-data$z)*((data$y-data$y0hat_sl)/(1-data$ps_sl))+data$y0hat_sl))-ate
    
    # Generalized random forest : causal forest
    grf <- grf::causal_forest(X = model.matrix(~ 0 + ., covs), 
                              Y = data$y, W = data$z, num.trees = 4000)
    aipw_grf <- grf::average_treatment_effect(grf, target.sample = "all", method = "AIPW")
    tmle_grf <- grf::average_treatment_effect(grf, target.sample = "all", method = "TMLE")
    
    # Store results: AIPW and TMLE
    results[Estimator == 'DR:AIPW:grf' & 
              DGP == i & Simulation == j ,'Bias'] <- aipw_grf["estimate"]-ate
    results[Estimator == 'DR:TMLE:grf' & 
              DGP == i & Simulation == j ,'Bias'] <- tmle_grf["estimate"]-ate
    
    
    #####################################
    ### --- PROGNOSTIC PROPENSITY --- ### 
    #####################################
    
    ############################
    # Linear Probability model # (Potentially misspecified)
    ############################
    
    # Run model and predict probabilities
    # For IPPW:LinLin
    data$pps_lpm <- lm(z ~ y1hat_lin + y0hat_lin, data = data) |> 
      predict(type = "response")
    
    ####################################
    # Kernel Regularized Least Squares # (Potentially misspecified)
    ####################################
    
    # Fit the model (incorrect link function)
    fit_gKRLS_lpm <- mgcv::gam(z ~ s(y1hat_krls, y0hat_krls, bs = "gKRLS"), data = data)
    # Predicted probabilities
    data$pps_krls_lpm <- predict.gam(fit_gKRLS_lpm, type = "response")
    # Fit the model (appropriate link function)
    start_time <- proc.time()
    fit_gKRLS_binom <- mgcv::gam(z ~ s(y1hat_krls, y0hat_krls, bs = "gKRLS"), 
                                 family = "binomial", data = data)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Krls:pps","time"] <- runtime[2]
    # Predicted probabilities
    data$pps_krls_binom <- predict.gam(fit_gKRLS_binom, type = "response")
    
    #################
    # Super Learner # Has to run each model separately
    #################
    
    # IPPW:LinLog
    task_pps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = c("y1hat_lin","y0hat_lin")
    )
    linlog_pps <- lrn_glm$train(task = task_pps)
    data$pps_linlog <- linlog_pps$predict()
    
    # IPPW:Lasso
    task_pps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = c("y1hat_lasso","y0hat_lasso")
    )
    lasso_pps <- lrn_lasso$train(task = task_pps)
    data$pps_lasso <- lasso_pps$predict()
    
    # IPPW:xgboost
    task_pps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = c("y1hat_xgb","y0hat_xgb"
      ))
    xgb_pps <- lrn_xgb$train(task = task_pps)
    data$pps_xgb <- xgb_pps$predict()
    
    # IPPW:earth
    task_pps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = c("y1hat_earth","y0hat_earth")
    )
    earth_pps <- lrn_earth$train(task = task_pps)
    data$pps_earth <- earth_pps$predict()
    
    # IPPW:SL
    # It has access to all the y1 and y0 hats from the models in SL
    # Shouldn't this be equal to the SL in general?
    # No! Because the SL would take zhats, no ydhats!
    # Also, here feeding a subset of learner's fits: 
    # lin, krls, xgb, earth
    task_pps <- sl3_Task$new(
      data = data,
      outcome = "z",
      outcome_type = "binomial",
      covariates = c("y1hat_lin","y0hat_lin",
                     "y1hat_lasso","y0hat_lasso",
                     "y1hat_krls","y0hat_krls",
                     "y1hat_xgb","y0hat_xgb",
                     "y1hat_earth","y0hat_earth")
    )
    start_time <- proc.time()
    sl_pps <- esl$train(task = task_pps)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "SL:pps","time"] <- runtime[2]
    data$pps_sl <- sl_pps$predict()
    
    
    ### --- MATCHING VERSION --- ###
    #"PPSM:LinLin","PPSM:LinLog","PPSM:xgboost",
    #"PPSM:earth","PPSM:Krls","PPSM:SL",
    
    
    ### --- WEIGHTING VERSION --- ###
    #"IPPW:LinLin","
    
    # IPPW: logistic propensity score
    ippw_linlog <- weightit(formula = cobalt::f.build("z", names(covs)),
                            ps = "pps_linlog",
                            stabilize = TRUE,
                            data = data)
    results[Estimator == 'IPPW:LinLog' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_linlog), data = data)$coefficients["z"]-ate
    
    # IPPW: Lasso pscore
    ippw_lasso <- weightit(formula = cobalt::f.build("z", names(covs)),
                           ps = "pps_lasso",
                           stabilize = TRUE,
                           data = data)
    results[Estimator == 'IPPW:Lasso' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_lasso), data = data)$coefficients["z"]-ate
    
    # IPPW: xgboost
    ippw_xgb <- weightit(formula = cobalt::f.build("z", names(covs)),
                         ps = "pps_xgb",
                         stabilize = TRUE,
                         data = data)
    results[Estimator == 'IPPW:xgboost' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_xgb), data = data)$coefficients["z"]-ate
    
    # IPPW: earth
    ippw_earth <- weightit(formula = cobalt::f.build("z", names(covs)),
                           ps = "pps_earth",
                           stabilize = TRUE,
                           data = data)
    results[Estimator == 'IPPW:earth' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_earth), data = data)$coefficients["z"]-ate
    
    # IPW: krls
    ippw_krls <- weightit(formula = cobalt::f.build("z", names(covs)),
                          ps = "pps_krls_binom",
                          stabilize = TRUE,
                          data = data)
    results[Estimator == 'IPPW:Krls' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_krls), data = data)$coefficients["z"]-ate
    
    # IPW: SL 
    start_time <- proc.time()
    ippw_sl <- weightit(formula = cobalt::f.build("z", names(covs)),
                        ps = "pps_sl",
                        stabilize = TRUE,
                        data = data)
    runtime <- proc.time() - start_time
    runtimes[DGP == i & Simulation == j & algorithm == "Bal:SL:pps","time"] <- runtime[2]
    results[Estimator == 'IPPW:SL' & 
              DGP == i & Simulation == j ,'Bias'] <- 
      lm(y ~ z, weights = get.w(ippw_sl), data = data)$coefficients["z"]-ate
    
    # Garbage collection at the end of iteration
    # Clean the workspace for next round
    gc(verbose = FALSE)
  } # End iterations over realizations
  print(paste("End iteration",j,"of dgp",i))
  data.table::fwrite(results, "results_dorie.csv")
  data.table::fwrite(runtimes, "runtimes_dorie.csv")
} # End iterations over DGPs

```


```{r, eval=FALSE}
library(ggplot2)
# Add DML results
results_dml <- read.csv("/Volumes/Fellows/geraldo/results_dml.csv")
results_dml <- results_dml |> dplyr::filter(DGP <= 8)

# Combine results of different estimators
results_clean <- rbind(results, results_dml) |> 
  dplyr::filter(!is.na(Bias)) |> 
  dplyr::mutate(type = dplyr::case_when(
    stringr::str_detect(Estimator, "Coeff") ~ "Coefficient",
    stringr::str_detect(Estimator, "Bal:Yd") ~ "Bal:Yd",
    stringr::str_detect(Estimator, "IPPW") ~ "IPPW",
    stringr::str_detect(Estimator, "G-comp") ~ "G-computation",
    stringr::str_detect(Estimator, "Bal") ~ "Balance",
    stringr::str_detect(Estimator, "DR") ~ "Doubly Robust",
    .default = "None"),
    learner = dplyr::case_when(
      grepl("SL", Estimator) ~ "SL",
      grepl("Lasso", Estimator) ~ "Lasso",
      grepl("LinLog", Estimator) ~ "Lin",
      grepl("LogLin", Estimator) ~ "Lin",
      grepl("xgboost", Estimator) ~ "XGB",
      grepl("earth", Estimator) ~ "Earth",
      grepl("Krls", Estimator) ~ "Krls",
      .default = "Other"))

results_clean$Estim_bias = with(results_clean, reorder(Estimator, Bias, mean))

# Bias distribution (by type)
results_clean |> 
  #dplyr::filter(!grepl("DR:DML", Estimator), Estimator!="DR:TMLE:grf") |> 
  ggplot(aes(y=Estim_rmse, x=Bias, color = type)) +
  geom_boxplot(outliers = FALSE) +
  #geom_jitter(alpha = 0.15, outliers = FALSE) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  theme_bw() +
  facet_wrap(~DGP)

# Bias distribution (by learner)
results_clean |> 
  #dplyr::filter(!grepl("DR:DML", Estimator), Estimator!="DR:TMLE:grf") |> 
  ggplot(aes(y=Estimator, x=Bias, color = learner)) +
  geom_boxplot() +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed")+
  theme_bw() +
  facet_wrap(~DGP)

# Bias average
results_clean |> 
  dplyr::group_by(Estimator, DGP) |> 
  dplyr::reframe(Bias = mean(Bias), type = type) |> 
  ggplot(aes(y=Estimator, x=Bias, color = type)) +
  geom_point() +
  theme_bw() +
  facet_wrap(~DGP)


# RMSE
results_clean |> 
  #dplyr::filter(!grepl("DR:DML", Estimator), Estimator!="DR:TMLE:grf") |> 
  dplyr::group_by(Estimator, type) |> 
  dplyr::summarise(RMSE = sqrt(mean(Bias^2))) |> 
  dplyr::arrange(RMSE)  |> 
  dplyr::mutate(Estimator = factor(Estimator, levels = Estimator)) |> 
  ggplot(aes(y=Estimator, x=RMSE, color = type)) +
  geom_segment(aes(yend = Estimator, xend = 0)) +
  geom_point(size = 4) +
  theme_bw()

# Bias-RMSE corr
results |> dplyr::filter(!is.na(Bias)) |> 
  dplyr::group_by(Estimator) |> 
  dplyr::summarize(RMSE = sqrt(mean(Bias^2)),
                   Bias = mean(Bias)) |> 
  ggplot(aes(x=Bias, y=RMSE)) +
  geom_point() +
  theme_bw()

```

Now we can run the simulations
```{r, eval=FALSE}

###########################
### --- SIMULATIONS --- ### 
###########################

# Create empty list to store data
data <- list()

# Iterate over different DGPs 
# (1:20), corresponding to the do-it-yourself part of the competition
dgp <- 1:n_dgps

for(i in dgp){
  
  # Combine different simulations of same DGP
  # (1:100), keep sim identifier
  
  # First, capture the list of files to be used
  file_list <- 
    list.files(paste0(path,"1"), # Switch to the i later
               pattern = "*.csv", 
               ignore.case = TRUE,
               full.names = FALSE)
  
  for(j in file_list){
    
    data[[j]] <- data.table::fread(
      file = paste0(path,"1/",j)
    )[, sim_id := j] # Add identifier to data
    
  }
  
  # Combine the list into a single data frame
  data <- data.table::rbindlist(data)
  
  
}  

```

### Treatment effect estimation

```{r}
# The estimators we are going to test are the following
# Balance covariate alone: IPW, PS-matching, Ebal, Tfbal
# Outcome model alone: Linear regression, Lin, GRF, Kbal
# DR models: AIPW, TMLE, DML
# Proposed estimators: 
# 1) Prognostic Balance (E(Y0))
# 2) Prognostic Propensity Score (E(D|Y0)) Balance
# 3) Prognostic 


fit_gKRLS1 <- mgcv::gam(fml1, data = data)
calculate_effects(fit_gKRLS1, variables = "z")
fit_gKRLS2 <- mgcv::gam(fml2, data = data)
calculate_effects(fit_gKRLS2, variables = "z")

# DR: Generalized random forest : causal forest
grf <- causal_forest(X = data[covs], Y = data$y, W = data$z)
aipw_grf <- average_treatment_effect(grf, target.sample = "all", method = "AIPW")
tmle_grf <- average_treatment_effect(grf, target.sample = "all", method = "TMLE")

# Save table with missing cases
data.table::fwrite(data,
                   file = paste0(path_results, "/table_data.csv"),
                   append = TRUE)

```
